{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f9b9ed-2e65-413b-b0d8-6cea81d6668d",
   "metadata": {},
   "source": [
    "# Informe de Exploración\n",
    "\n",
    "En este informe explicaremos algunas de las funcionalidades de los módulos de Python enfocados a la estadística y la Ciencia de Datos, `sklearn` y `statsmodels`. Lo anterior lo realizaremos mediante la descripción de un proyecto de Análisis de Datos, en el cual hay $2$ objetivos, realizar un Agrupamiento o *Clustering* para un conjunto de datos y realizar un estudio descriptivo de la relación entre $2$ variables dentro de otro conjunto de datos, mediante algun modelo supervizado de regresión. Empezamos por dar una breve descripción de ambos módulos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cc641-d3e2-4c78-b267-f5c343ae6bb1",
   "metadata": {},
   "source": [
    "## 1. Sklearn y Statsmodels\n",
    "\n",
    "`sklearn` tambien conocido como `scikit-learn` es una biblioteca de Python, compatible con otros modulos como `Pandas`,`NumPy`,`statsmodels` e incluso `matplotlib`. Esta biblioteca proporciona una amplia variedad de herramientas y algoritmos para la construcción, evaluación y despliegue de modelos de aprendizaje automático. Tambien provee métodos de gran utilidad para preparación de Datos tales como preprocesamiento, métricas y reducción dimencional y ofrece tambien modelos para el estudio de clasificación, regresión y clustering.\n",
    "\n",
    "Por otro lado `statsmodels` es una biblioteca de Python especializada en estadísticas y modelado estadístico. Tambien posee compatibilidad con `Pandas` y `NumPy`. A diferencia de scikit-learn, que se centra en el aprendizaje automático y la predicción, statsmodels se dedica principalmente al análisis estadístico clásico, la estimación de parámetros, la inferencia estadística y pruebas de Hipótesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2afd4-96d4-4173-b0d3-e416d9aae849",
   "metadata": {},
   "source": [
    "## 2. Sobre los Proyectos\n",
    "Para ambos estudios exploratorios tomamos los datos de \"jugete\" que nos provee el modulo `sklearn.datasets`, el primero un grupo de datos de $3$ dimensiones conocido como *Rollo Suizo* y el otro una tabla de datos con varias propiedades de Vinos, ambos datos se encuentran en formato `JSON`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d4c03-5f51-4ad0-8368-7324988d3991",
   "metadata": {},
   "source": [
    "### Clustering de Datos:\n",
    "\n",
    "El primer proyecto toma el *Rollo Suizo* generado por medio de `sklearn` y lo reduce de dimensión utilizando el Análisis de Componentes Principales (PCA) atravez de el método predefinido de nuevo en `sklearn`, a uno de $2$ dimensiones. Despues, construimos $2$ modelos distintos de agrupamiento mediante `sklearn`:\n",
    "\n",
    "- El primero el método *Mean-shift*: el cual es un algoritmo que funciona actualizando los candidatos para centroides para que sean la media de los puntos dentro de una región determinada. Luego, estos candidatos se filtran en una etapa de posprocesamiento para eliminar casi duplicados y formar el conjunto final de centroides. El resultado son un conjunto de *Manchas* en una densidad suave de muestras, que seran nuestros respectivos *Clusters*. A diferencia de *K-Means* no requiere especificar el número de clusters de antemano y puede detectar clusters de formas y tamaños arbitrarios.\n",
    "\n",
    "- El segundo es el método de *Agrupación espectral*: realiza una incrustación de baja dimensión de la matriz de afinidad entre muestras, seguida de la agrupación, por ejemplo, mediante *K-Means*, de los componentes de los vectores propios en el espacio de baja dimensión. Es especialmente eficiente desde el punto de vista computacional si la matriz de afinidad es escasa, aunque necesita que se pre-establescan el número de clusters.\n",
    "\n",
    "Junto con esto realizamos las graficas con los puntos asignado por ambos métodos. Para determinar que tan correctamente se comportan dichos modelos, es decir, para realizar la validación de estos, se utilizo el método `softmax()` (en donde evidentemente se separaron los datos entre testeo y prueba), esto atravez de la **Regresión Logística** de `sklearn.linear_model` y la métrica de la misma biblioteca `accuracy_score()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f5d09-fecd-43de-ac9b-36f5a663446d",
   "metadata": {},
   "source": [
    "### Regresiones para Descripciones:\n",
    "\n",
    "En cuanto a este segundo proyecto tomamos los datos pre-establecidos de `load_wine` en formato `JSON` en el pre-procesamiento transforme dichos datos en un `DataFrame` de Pandas y extraje las variables que queria estudiar como se relacionaban, para ello tambien normalice dicho `DataFrame`, dado que las variables que me interesaba estudiar (la Cantidad de Alcohol de Vino y el Color del Vino) no parecian tener una distribución normal o Gaussiana utilice el método `MinMaxScaler` que escalo dichas variables entre $0$ y $1$, esto tiene una razón de ser y es que en dicha escala los modelos de **Regresión Clásica** y **Regresión con Soporte Vectorial (SRV)** funcionan mejor. En total estudie $3$ tipos de regresiones, donde mi Variable Independiente era el **Alcohol** del vino y mi Variable Dependiente era el **Color** del vino, como sigue:\n",
    "\n",
    "- Modelo de Regresión Lineal: Para este utilice el algoritmo clásico de `statsmodels`: `modelo_lineal=sm.OLS().fit()` en el cual se establece la pendiente mediante `modelo_lineal.params`. Tambien, utilice `summary()` para analizar las propiedades de **Coeficientes**, **P-Valores** y **R-Cuadrado**.\n",
    "\n",
    "- Modelo de Regresión Polinomial: est caso es interesante pues para lograrlo utilice de manera conjunta ambos modulos que debia explorar; con `sklearn` encontre los coeficientes necesarios para el polinomio (que estableci, debia ser de grado $3$) y despues utilice `statsmodels` para construir el modelo de manera similar a la regresión lineal. Al igual que antes obtuve y estudie los **Coeficientes**, **P-Valores** y **R-Cuadrado**.\n",
    "\n",
    "- Modelo de Funcion de Base Radial (RBF): A diferencia de los anteriores este modelo se construyo totalmente apartir de `sklearn`. Este tipo de regreción utiliza el radio que tienen los puntos con respecto a unos centros, para ello fue necesario usar la *Regresión con Soporte Vectorial (SVR)* que tiene como base las *Máquinas de Soporte Vectorial (SVM)* y no pude obtener sus propiedades estadísticas por medio de `summary()`. En vez de eso, utilice la Prueba de Hipótesis (en este caso la Hipótesis Nula (H$0$)) **t-Student** para comparar los valores generados por el modelo con la Variable Dependiente Color, con lo cual pude obtener **P-Valores** y **R-Cuadrado** aunque no representen exactamente lo mismo que en las regresiones anteriores.\n",
    "\n",
    "Además en todos ellos realice la comparación entre la gráfica de disperción de los valores reales junto con la respectiva regresión utilizando `matplotlib.pyplot`. Por último evalue la calidad de mis modelos mediante la Función de Pérdida *Hubber Loss*, aunque debido a la naturaleza de esta tambien calcule la pérdida con el *Error Cuadratico Medio (MSE)* y el *Error Absoluto Medio (MAE)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbaf6bb-ade3-4d82-84aa-8fef4b558d8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Resultados y Concluciones\n",
    "\n",
    "En el proceso de Clustering, según la prueba `softmax` obtuvimos que el mejor método de agrupamiento fue el de *Agrupación Espectral*, en el cual aproximadamente $0$ fueron considerados ruido y donde habian $7$ Clusters o Grupos. Por medio de *K-Means* los Clusters se encontraban entre $5$ y $9$ (esto debido a que el *Rollo Suizo* cambiaba cada vez que reiniciaba el Kernel), lo que corrobora los Clusters del Espectral.\n",
    "\n",
    "Para describir la relación entre el Alcohol del vino y su Color, comparamos (por razones que explicamos en la Exploración de Modulos) la pérdida de la Función *Hubber Loss*, con lo cual resulto que la mejor regresión descriptiva es la **Regresión Polinomial**. Sin embargo, los coeficientes de este método no son representativos de los datos si los tomamos por separado, además este modelo no provee de información satisfactoria en cuanto a la variación del Color con respecto al Alcohol. \n",
    "\n",
    "Para términar puedo afirmar que los paquetes de `statsmodels` y `sklearn` en problemas de Ciencia de Datos más que limitarse a Estadística Clásica o Machine Learning, son sorprendentemente complementarios el uno con el otro al punto que son compatibles entre sí para ciertos métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc3788-117b-4750-8e46-e79095bfcf1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Ejemplos de Código Relevante\n",
    "\n",
    "Existen varios ejemplos interesantes, veamos algunos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc5ab5-e06a-4609-bf05-93c6a7f13c3b",
   "metadata": {},
   "source": [
    "- Ejemplo de Integración entre `statsmodels` y `sklearn` para la Regresión Polinomial:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a63037f6-e861-4546-b507-9ebdd0e506cb",
   "metadata": {},
   "source": [
    "# Utilizando sklearn para mejorar nuestro modelo:\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "caracteristicas_polinomiales = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Construyendo el modelo:\n",
    "Alcohol_Polinomial = caracteristicas_polinomiales.fit_transform(Alcohol_2)\n",
    "modelo_polinomial = sm.OLS(Color, Alcohol_Polinomial).fit()\n",
    "\n",
    "# Obtén los coeficientes del modelo\n",
    "coeficientes_polinomiales = modelo_polinomial.params\n",
    "\n",
    "# Realicemos la Predicción con los Datos:\n",
    "Prediccion_Polinomial = modelo_polinomial.predict(Alcohol_Polinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae15ad-c5c8-48eb-96b5-f6cfc32d328e",
   "metadata": {},
   "source": [
    "- Ejemplo de `statmodels` para estudiar las propiedades estadísticas (*t-Student*) de una regresión apoyada por soporte vectorial de `sklearn` (*SVR*):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95d0fdfa-581d-4d8e-9515-4d5aaa5644c5",
   "metadata": {},
   "source": [
    "# Primero importemos el método de Máquina de Soporte Vectorial para Regresiones (SVR):\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Crea un modelo de regresión RBF con SVM\n",
    "modelo_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "# Entrena el modelo\n",
    "modelo_rbf.fit(Alcohol, Color)\n",
    "\n",
    "# Predice valores\n",
    "Prediccion_rbf = modelo_rbf.predict(Alcohol)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faf5482f-e35e-4ba6-a86f-583d1acc4833",
   "metadata": {},
   "source": [
    "# Convierte 'Prediccion_rbf' en un objeto DataFrame de pandas\n",
    "predicciones_df= pd.DataFrame({'Prediccion_rbf': Prediccion_rbf})\n",
    "\n",
    "# Concatena 'Prediccion_rbf' y 'Color' en un DataFrame\n",
    "datos = pd.concat([predicciones_df, Color], axis=1)\n",
    "\n",
    "# Realiza la prueba t-Student\n",
    "modelo_t = sm.OLS.from_formula(\"Color ~ Prediccion_rbf\", data=datos).fit()\n",
    "\n",
    "# Obtén los resultados de la prueba t-Student\n",
    "resultados_t = modelo_t.summary()\n",
    "\n",
    "# Imprime los resultados\n",
    "print(resultados_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34880e-f574-4d06-bedb-c73dc751f46e",
   "metadata": {},
   "source": [
    "- Modelación de `softmax()` con `sklearn`:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6277a5f1-0621-4a49-9ff2-b2fcf1ec32be",
   "metadata": {},
   "source": [
    "# Modelo 'softmax':\n",
    "modelo_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Evaluación del modelo Mean-Shift:\n",
    "prediccion_mean = modelo_softmax.fit(rollo_mean_ent, etiqueta_mean_ent).predict(rollo_mean_pru)\n",
    "precision_mean = accuracy_score(etiqueta_mean_pru, prediccion_mean)\n",
    "\n",
    "# Evaluación del modelo de Agrupamiento Espectral:\n",
    "prediccion_espectral = modelo_softmax.fit(rollo_espectro_ent, etiqueta_espectro_ent).predict(rollo_espectro_pru)\n",
    "precision_espectral = accuracy_score(etiqueta_espectro_pru, prediccion_espectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba57b8d-5faf-4e46-98bc-d9a41bf06447",
   "metadata": {},
   "source": [
    "- Usando `sklearn` para construir una Función *Hubber Loss*:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaf35d10-d5dd-44f6-bc83-42a6c9d93436",
   "metadata": {},
   "source": [
    "# Calcular la pérdida Hubber utilizando sklearn:\n",
    "def hubber(y_true, y_pred, delta=(rango_Color+tendencia_Color)):\n",
    "    residual = y_true - y_pred\n",
    "    huber_condition = np.abs(residual) <= delta\n",
    "    loss = np.where(huber_condition, 0.5 * residual ** 2, delta * (np.abs(residual) - 0.5 * delta))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ead29-6449-423e-8ff8-1bef36ec174b",
   "metadata": {},
   "source": [
    "## 5. Problemas durante la Exploración:\n",
    "En mi caso se presentaro bastantes problemas durante el desarrollo de esta exploración, citare los que consiero más relevantes:\n",
    "\n",
    "- Al ser la segunda vez que trato con datos en formato JSON, al principio se presentaron algunas dificultades para el pre-procesamiento, pero al entender la dinámica de llaves y los meta-datos, logre superar estos inconvenientes.\n",
    "\n",
    "- En el clustering, al principio intente utilizar los modelos de DBSCAN (agrupamiento espacial basado en densidad de aplicaciones con ruido) y la versión automatica de la misma OPTICS, sin embargo estos o metian todos los puntos en $1$ solo Grupo o los interpretaba todos como Ruido. Esto sucede debido a que dichos métodos son mas apropiados para pocos Grupos con Muchisímos datos (ideal para imagenes).\n",
    "\n",
    "- Al principio mis gráficas eran horribles y no entendia si se debia al modelo o al código para gráficación, resulto esto último pues para regresiones no lineales se deben organizar los datos para que `matplotlib` los gráfique correctamente.\n",
    "\n",
    "- Realmente no entiendo porque la regreción polinomial, apesar de que la configure para ser de grado $3$ tiene $9$ coeficiente. Supongo que esto se debe a que `PolynomialFeatures()` de `sklearn.preprocessing` es el que produce aquello.\n",
    "\n",
    "- Algo que no se exactamente como hacer y me gustaría es configurar el código para que ajuste los parámetros del modelo de tal manera que obtengan el menor error posible en la métrica de valuación. Supongo que esto se hace mediante Redes Neuronales y Machine Learning aunque eso se salga de los límites de esta tarea. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
